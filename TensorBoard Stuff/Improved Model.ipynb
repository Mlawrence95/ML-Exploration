{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which versions of Tensorflow and Sci-kit learn are in use so that documentation is easier to use when we get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.11.0', '0.19.1')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab MNIST data using tensorflow (this may take a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the size of each dataset. It looks like the train:test ratio is 6:1, with the test data being one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of labels to ensure that we are using a balanced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Label')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts().reset_index().hist('index')\n",
    "plt.title(\"Distribution of labels\")\n",
    "plt.xlabel('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the y data has many classes, let's one-hot encode it with sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneH = OneHotEncoder()\n",
    "oneH.fit(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohYTest = oneH.transform(y_test.reshape(-1,1)).toarray()\n",
    "ohYTrain = oneH.transform(y_train.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test labels are now stored in sparse matrices. A quick unit test below ensures that the transformation was performed correctly. The printed data should support your intuition as to what the test is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed label:\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "Original label:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed label:\")\n",
    "print(ohYTrain[0])\n",
    "print()\n",
    "print(\"Original label:\")\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_train.shape[0]):\n",
    "    assert np.argmax(ohYTrain[i]) == y_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for a sanity check, let's make sure the first data point is actually a 5. Matplotlib supports plotting images, so let's leverage its power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADolJREFUeJzt3X2MXOV1x/HfyXq9jo1JvHVsHOJgxzgBYhqTjgzICFwhXKdCMqgCYkWRQ5M4LzgprStBraq4FancKiF1CUVamq1tifcEiv+gSZAVAVFhy+IQXuLwErMli7e7mA3YEOKX3dM/9m60MTvPrGfuzJ3d8/1I1szcc+/co4Hf3pl55t7H3F0A4nlP0Q0AKAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1LRG7my6tfkMzWrkLoFQfqu3dcQP20TWrSn8ZrZG0jZJLZL+3d23ptafoVk61y6uZZcAErp894TXrfptv5m1SLpF0qcknSVpnZmdVe3zAWisWj7zr5D0krvvc/cjku6StDaftgDUWy3hP1XSr8Y87s2W/R4z22Bm3WbWfVSHa9gdgDzVEv7xvlR41/nB7t7h7iV3L7WqrYbdAchTLeHvlbRwzOMPSdpfWzsAGqWW8D8haamZLTaz6ZI+LWlXPm0BqLeqh/rc/ZiZbZT0Q40M9XW6+3O5dQagrmoa53f3ByU9mFMvABqIn/cCQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVE2z9JpZj6RDkoYkHXP3Uh5NIT82Lf2fuOUDc+u6/+f/elHZ2tDM4eS2py0ZSNZnftWS9f+7aXrZ2p7S3cltDwy9nayfe++mZP30v3o8WW8GNYU/88fufiCH5wHQQLztB4KqNfwu6Udm9qSZbcijIQCNUevb/pXuvt/M5kl6yMx+4e6PjF0h+6OwQZJmaGaNuwOQl5qO/O6+P7sdkHS/pBXjrNPh7iV3L7WqrZbdAchR1eE3s1lmNnv0vqTVkp7NqzEA9VXL2/75ku43s9HnucPdf5BLVwDqrurwu/s+SZ/IsZcpq+XMpcm6t7Um6/sven+y/s555cek29+XHq9+9BPp8e4i/ddvZifr//SdNcl619l3lK29fPSd5LZb+y9J1j/4qCfrkwFDfUBQhB8IivADQRF+ICjCDwRF+IGg8jirL7yhVZ9M1m/afkuy/tHW8qeeTmVHfShZ/7ubP5esT3s7Pdx2/r0by9Zmv3osuW3bgfRQ4MzurmR9MuDIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc6fg7bn9yfrT/52YbL+0db+PNvJ1aa+85L1fW+lL/29fcn3ytbeHE6P08//1/9O1utp8p+wWxlHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IytwbN6J5srX7uXZxw/bXLAavPj9ZP7gmfXntlqdPStZ/9tWbT7inUTce+MNk/YmL0uP4Q2+8maz7+eWv7t7z9eSmWrzuZ+kV8C5dvlsHfTA9d3mGIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFVxnN/MOiVdKmnA3Zdly9ol3S1pkaQeSVe6+68r7SzqOH8lLXP/IFkfen0wWX/5jvJj9c9d2JncdsU/fi1Zn3dLcefU48TlPc6/XdLxE6FfL2m3uy+VtDt7DGASqRh+d39E0vGHnrWSdmT3d0i6LOe+ANRZtZ/557t7nyRlt/PyawlAI9T9Gn5mtkHSBkmaoZn13h2ACar2yN9vZgskKbsdKLeiu3e4e8ndS61qq3J3APJWbfh3SVqf3V8v6YF82gHQKBXDb2Z3SnpM0sfMrNfMPi9pq6RLzOxFSZdkjwFMIhU/87v7ujIlBuxzMnTg9Zq2P3pwetXbfvwzP0/WX7u1Jf0Ew0NV7xvF4hd+QFCEHwiK8ANBEX4gKMIPBEX4gaCYonsKOPO6F8rWrj47PSL7H6ftTtYvuuKaZH323Y8n62heHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+aeA1DTZr3/lzOS2r+x6J1m//sadyfrfXHl5su4/fV/Z2sJvPJbcVg2cPj4ijvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTFKbrzxBTdzWfwz89P1m+/4ZvJ+uJpM6re98d3bkzWl97Wl6wf29dT9b6nqryn6AYwBRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAVx/nNrFPSpZIG3H1ZtmyLpC9Kei1bbbO7P1hpZ4zzTz6+cnmyfvLW3mT9zo/8sOp9n/HjLyTrH/v78tcxkKShF/dVve/JKu9x/u2S1oyz/Nvuvjz7VzH4AJpLxfC7+yOSBhvQC4AGquUz/0Yze9rMOs1sTm4dAWiIasN/q6QlkpZL6pP0rXIrmtkGM+s2s+6jOlzl7gDkrarwu3u/uw+5+7Ck2yStSKzb4e4ldy+1qq3aPgHkrKrwm9mCMQ8vl/RsPu0AaJSKl+42szslrZI018x6Jd0gaZWZLZfkknokfamOPQKoA87nR01a5s9L1vdfdXrZWtd125LbvqfCG9PPvLw6WX/zgteT9amI8/kBVET4gaAIPxAU4QeCIvxAUIQfCIqhPhTmnt70FN0zbXqy/hs/kqxf+rVryz/3/V3JbScrhvoAVET4gaAIPxAU4QeCIvxAUIQfCIrwA0FVPJ8fsQ1fkL509y+vSE/RvWx5T9lapXH8Sm4ePCdZn/lAd03PP9Vx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnn+KstCxZf+Hr6bH221buSNYvnJE+p74Wh/1osv744OL0Ewz35djN1MORHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2YLJe2UdIqkYUkd7r7NzNol3S1pkaQeSVe6+6/r12pc0xaflqz/8uoPlq1tuequ5LZ/dtKBqnrKw+b+UrL+8LbzkvU5O9LX/UfaRI78xyRtcvczJZ0n6RozO0vS9ZJ2u/tSSbuzxwAmiYrhd/c+d9+T3T8kaa+kUyWtlTT6868dki6rV5MA8ndCn/nNbJGkcyR1SZrv7n3SyB8ISfPybg5A/Uw4/GZ2kqTvS7rW3Q+ewHYbzKzbzLqP6nA1PQKogwmF38xaNRL82939vmxxv5ktyOoLJA2Mt627d7h7yd1LrWrLo2cAOagYfjMzSd+VtNfdbxpT2iVpfXZ/vaQH8m8PQL1M5JTelZI+K+kZM3sqW7ZZ0lZJ95jZ5yW9IumK+rQ4+U1b9OFk/c0/WpCsX/UPP0jWv/z++5L1etrUlx6Oe+zfyg/ntW//n+S2c4YZyquniuF3959IKjff98X5tgOgUfiFHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt09QdMWnFK2Ntg5K7ntVxY/nKyvm91fVU952PjqBcn6nlvTU3TP/d6zyXr7IcbqmxVHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKsw4/5E/SV8m+shfDibrm09/sGxt9XvfrqqnvPQPvVO2duGuTcltz/jbXyTr7W+kx+mHk1U0M478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUmHH+nsvSf+deOPveuu37ljeWJOvbHl6drNtQuSunjzjjxpfL1pb2dyW3HUpWMZVx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMzd0yuYLZS0U9IpGjl9u8Pdt5nZFklflPRatupmdy9/0rukk63dzzVm9Qbqpct366APpn8YkpnIj3yOSdrk7nvMbLakJ83soaz2bXf/ZrWNAihOxfC7e5+kvuz+ITPbK+nUejcGoL5O6DO/mS2SdI6k0d+MbjSzp82s08zmlNlmg5l1m1n3UR2uqVkA+Zlw+M3sJEnfl3Stux+UdKukJZKWa+SdwbfG287dO9y95O6lVrXl0DKAPEwo/GbWqpHg3+7u90mSu/e7+5C7D0u6TdKK+rUJIG8Vw29mJum7kva6+01jli8Ys9rlktLTtQJoKhP5tn+lpM9KesbMnsqWbZa0zsyWS3JJPZK+VJcOAdTFRL7t/4mk8cYNk2P6AJobv/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfHS3bnuzOw1Sf87ZtFcSQca1sCJadbemrUvid6qlWdvp7n7ByayYkPD/66dm3W7e6mwBhKatbdm7Uuit2oV1Rtv+4GgCD8QVNHh7yh4/ynN2luz9iXRW7UK6a3Qz/wAilP0kR9AQQoJv5mtMbPnzewlM7u+iB7KMbMeM3vGzJ4ys+6Ce+k0swEze3bMsnYze8jMXsxux50mraDetpjZq9lr95SZ/WlBvS00sx+b2V4ze87M/iJbXuhrl+irkNet4W/7zaxF0guSLpHUK+kJSevc/ecNbaQMM+uRVHL3wseEzexCSW9J2unuy7Jl/yxp0N23Zn8457j7dU3S2xZJbxU9c3M2ocyCsTNLS7pM0udU4GuX6OtKFfC6FXHkXyHpJXff5+5HJN0laW0BfTQ9d39E0uBxi9dK2pHd36GR/3karkxvTcHd+9x9T3b/kKTRmaULfe0SfRWiiPCfKulXYx73qrmm/HZJPzKzJ81sQ9HNjGN+Nm366PTp8wru53gVZ25upONmlm6a166aGa/zVkT4x5v9p5mGHFa6+yclfUrSNdnbW0zMhGZubpRxZpZuCtXOeJ23IsLfK2nhmMcfkrS/gD7G5e77s9sBSfer+WYf7h+dJDW7HSi4n99pppmbx5tZWk3w2jXTjNdFhP8JSUvNbLGZTZf0aUm7CujjXcxsVvZFjMxslqTVar7Zh3dJWp/dXy/pgQJ7+T3NMnNzuZmlVfBr12wzXhfyI59sKONfJLVI6nT3bzS8iXGY2Uc0crSXRiYxvaPI3szsTkmrNHLWV7+kGyT9p6R7JH1Y0iuSrnD3hn/xVqa3VRp56/q7mZtHP2M3uLcLJD0q6RlJw9nizRr5fF3Ya5foa50KeN34hR8QFL/wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8D6+E2hIAP97kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great. Let's examine the shape of each image's matrix to plan for our modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our model in tensorflow, including ensuring that the computational graph is ready to be used from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graveyard of Ideas ##\n",
    "\n",
    "#  with tf.name_scope(\"Dense_L2_Reg\"):\n",
    "#         model.add(layers.Dense(100, activation=tf.nn.))\n",
    "#         model.add(layers.Dropout(0.10))\n",
    "        \n",
    "# with tf.name_scope(\"Batch_Norm\"):\n",
    "#         model.add(layers.BatchNormalization())\n",
    "    \n",
    "# with tf.name_scope(\"Dense_Leaky\"):\n",
    "#     model.add(layers.Dense(2000, activation=tf.nn.leaky_relu))\n",
    "#     model.add(layers.Dropout(0.10))\n",
    "\n",
    "    \n",
    "#     with tf.name_scope(\"Dense_Leaky\"):\n",
    "#         model.add(layers.Dense(800))\n",
    "#         model.add(layers.BatchNormalization())\n",
    "#         model.add(layers.LeakyReLU())\n",
    "#         model.add(layers.Dropout(0.10))\n",
    "\n",
    "#     with tf.name_scope(\"Dense_Leaky\"):\n",
    "#         model.add(layers.Dense(200))\n",
    "#         model.add(layers.BatchNormalization())\n",
    "#         model.add(layers.LeakyReLU())\n",
    "#         model.add(layers.Dropout(0.10))\n",
    "\n",
    "#     with tf.name_scope(\"Dense_Leaky\"):\n",
    "#         model.add(layers.Dense(100))\n",
    "#         model.add(layers.BatchNormalization())\n",
    "#         model.add(layers.LeakyReLU())\n",
    "#         model.add(layers.Dropout(0.10))\n",
    "\n",
    "# with tf.name_scope(\"Image_Augmentation\"):\n",
    "#     imAug = ImageDataGenerator(zoom_range = 0.1,\n",
    "#                             height_shift_range = 0.1,\n",
    "#                             width_shift_range = 0.1,\n",
    "#                             rotation_range = 10)\n",
    "\n",
    "#     model.fit_generator(imAug.flow(x_train, ohYTrain, batch_size=batchSize),\n",
    "#                            steps_per_epoch=500,\n",
    "#                            epochs=epochs, #Increase this when not on Kaggle kernel\n",
    "#                            verbose=2,  #1 for ETA, 0 for silent\n",
    "#                            validation_data=(x_test[:400,:], ohYTest[:400,:]), #For speed\n",
    "#                            callbacks=[tensorBoard, adaptLR])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 758)               595030    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 758)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               97152     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 697,280\n",
      "Trainable params: 696,960\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with tf.name_scope(\"Improved_Model\"):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    with tf.name_scope(\"Input_Flatten\"):\n",
    "        model.add(layers.Flatten(input_shape=(28,28,)))\n",
    "\n",
    "    with tf.name_scope(\"Dense_Leaky\"):\n",
    "        model.add(layers.Dense(758))\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "    with tf.name_scope(\"Dense_Leaky\"):\n",
    "        model.add(layers.Dense(128) )\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.15))\n",
    "        \n",
    "    with tf.name_scope(\"Dense_Leaky\"):\n",
    "        model.add(layers.Dense(32))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.10))\n",
    "\n",
    "    with tf.name_scope(\"Output\"):\n",
    "        model.add(layers.Dense(output_dim = 10, name=\"Output\"))\n",
    "        model.add(layers.Softmax())\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"tBoard_Setup\"):\n",
    "    tensorBoard = keras.callbacks.TensorBoard(log_dir=\"./improved_logs\", histogram_freq=1, write_graph=True)\n",
    "    tensorBoard.set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Adaptive_Learning_Rate\"):\n",
    "    \n",
    "    adaptLR = keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path for Tensorboard Logs:\n",
      "/Users/mikelawrence/Documents/School/Data\\ X/HW/HW\\ 10/improved_logs\n"
     ]
    }
   ],
   "source": [
    "logDir = \"improved_logs\"\n",
    "logPath = (os.getcwd() + \"/\" + logDir).replace(\" \", \"\\\\ \")\n",
    "print(\"Path for Tensorboard Logs:\")\n",
    "print(logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Compilation\"):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 20\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.3093 - acc: 0.9115 - val_loss: 0.1589 - val_acc: 0.9508\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009000000000000001.\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1626 - acc: 0.9506 - val_loss: 0.1169 - val_acc: 0.9627\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0008100000000000001.\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1282 - acc: 0.9607 - val_loss: 0.0859 - val_acc: 0.9726\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007290000000000002.\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1010 - acc: 0.9690 - val_loss: 0.0840 - val_acc: 0.9747\n",
      "Epoch 5/25\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0006561000000000001.\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.0869 - acc: 0.9725 - val_loss: 0.0710 - val_acc: 0.9777\n",
      "Epoch 6/25\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00059049.\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0739 - acc: 0.9767 - val_loss: 0.0721 - val_acc: 0.9777\n",
      "Epoch 7/25\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000531441.\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 0.0626 - acc: 0.9800 - val_loss: 0.0729 - val_acc: 0.9772\n",
      "Epoch 8/25\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004782969000000001.\n",
      "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0544 - acc: 0.9822 - val_loss: 0.0628 - val_acc: 0.9805\n",
      "Epoch 9/25\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004304672100000001.\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.0504 - acc: 0.9840 - val_loss: 0.0575 - val_acc: 0.9842\n",
      "Epoch 10/25\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0003874204890000001.\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 0.0399 - acc: 0.9874 - val_loss: 0.0622 - val_acc: 0.9815\n",
      "Epoch 11/25\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0003486784401000001.\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0354 - acc: 0.9885 - val_loss: 0.0640 - val_acc: 0.9824\n",
      "Epoch 12/25\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0003138105960900001.\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0633 - val_acc: 0.9825\n",
      "Epoch 13/25\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002824295364810001.\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0278 - acc: 0.9911 - val_loss: 0.0598 - val_acc: 0.9829\n",
      "Epoch 14/25\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002541865828329001.\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0252 - acc: 0.9918 - val_loss: 0.0550 - val_acc: 0.9839\n",
      "Epoch 15/25\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0002287679245496101.\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0215 - acc: 0.9933 - val_loss: 0.0549 - val_acc: 0.9840\n",
      "Epoch 16/25\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0002058911320946491.\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0607 - val_acc: 0.9830\n",
      "Epoch 17/25\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00018530201888518417.\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0580 - val_acc: 0.9842\n",
      "Epoch 18/25\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00016677181699666576.\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0158 - acc: 0.9946 - val_loss: 0.0572 - val_acc: 0.9849\n",
      "Epoch 19/25\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00015009463529699917.\n",
      "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0575 - val_acc: 0.9855\n",
      "Epoch 20/25\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0001350851717672993.\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0624 - val_acc: 0.9842\n",
      "Epoch 21/25\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00012157665459056935.\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0603 - val_acc: 0.9842\n",
      "Epoch 22/25\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00010941898913151242.\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0588 - val_acc: 0.9854\n",
      "Epoch 23/25\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 9.847709021836118e-05.\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0101 - acc: 0.9969 - val_loss: 0.0586 - val_acc: 0.9850\n",
      "Epoch 24/25\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 8.862938119652506e-05.\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0580 - val_acc: 0.9849\n",
      "Epoch 25/25\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 7.976644307687256e-05.\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0570 - val_acc: 0.9861\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Training'):\n",
    "    model.fit(x=x_train, y=ohYTrain, verbose=1, shuffle=True, batch_size=batchSize, nb_epoch=epochs,\n",
    "              callbacks=[tensorBoard, adaptLR], validation_data=(x_test, ohYTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Loss and Accuracy are:\n",
      "[0.0010037639418595669, 0.99985]\n",
      "\n",
      "Final Test Loss and Accuracy are:\n",
      "[0.05703200456253835, 0.9861]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Evaluation\"):\n",
    "    print(\"Final Train Loss and Accuracy are:\")\n",
    "    print(model.evaluate(x_train, ohYTrain, verbose=0))\n",
    "    print()\n",
    "    print(\"Final Test Loss and Accuracy are:\")\n",
    "    print(model.evaluate(x_test, ohYTest, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikelawrence/Documents/School/Data\\ X/HW/HW\\ 10/improved_logs\n"
     ]
    }
   ],
   "source": [
    "print(logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/Users/mikelawrence/Documents/School/Data\\ X/HW/HW\\ 10/improved_logs #--port=6005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
